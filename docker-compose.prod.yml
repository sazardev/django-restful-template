# ============================================================================
# DJANGO RESTFUL LOGISTICS TEMPLATE - PRODUCTION DOCKER COMPOSE
# ============================================================================
# Configuración completa para producción con todos los servicios
# Uso: docker-compose -f docker-compose.prod.yml up -d

version: "3.9"

services:
  # ============================================================================
  # NGINX - Reverse Proxy & Load Balancer
  # ============================================================================
  nginx:
    image: nginx:1.21-alpine
    container_name: logistics_nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - static_volume:/var/www/staticfiles:ro
      - media_volume:/var/www/media:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      web:
        condition: service_healthy
      websocket:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - logistics_network
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost/health/",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ============================================================================
  # DJANGO WEB APP - Main Application
  # ============================================================================
  web:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILD_ENV=production
    container_name: logistics_web
    command: |
      sh -c "
        echo 'Starting Django application...'
        python manage.py collectstatic --noinput
        python manage.py migrate --noinput
        if [ \"$$LOAD_TEST_DATA\" = \"true\" ]; then
          echo 'Loading test data...'
          python manage.py setup_initial_data
        fi
        gunicorn config.wsgi:application \
          --bind 0.0.0.0:8000 \
          --workers 4 \
          --worker-class gevent \
          --worker-connections 1000 \
          --max-requests 1000 \
          --max-requests-jitter 50 \
          --timeout 30 \
          --keep-alive 2 \
          --log-level info \
          --access-logfile /app/logs/gunicorn-access.log \
          --error-logfile /app/logs/gunicorn-error.log
      "
    volumes:
      - static_volume:/app/staticfiles
      - media_volume:/app/media
      - ./logs:/app/logs
      - ./backups:/app/backups
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    env_file:
      - .env.prod
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - logistics_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.web.rule=Host(`api.logistics.local`)"

  # ============================================================================
  # DJANGO WEBSOCKET SERVER - Real-time Communications
  # ============================================================================
  websocket:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILD_ENV=production
    container_name: logistics_websocket
    command: |
      sh -c "
        echo 'Starting WebSocket server...'
        daphne -b 0.0.0.0 -p 8001 config.asgi:application
      "
    volumes:
      - ./logs:/app/logs
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    env_file:
      - .env.prod
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - logistics_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/ws/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ============================================================================
  # CELERY WORKER - Background Tasks
  # ============================================================================
  celery:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILD_ENV=production
    container_name: logistics_celery
    command: |
      sh -c "
        echo 'Starting Celery worker...'
        celery -A config worker \
          --loglevel=info \
          --concurrency=4 \
          --prefetch-multiplier=1 \
          --max-tasks-per-child=1000 \
          --logfile=/app/logs/celery-worker.log
      "
    volumes:
      - ./logs:/app/logs
      - media_volume:/app/media
      - ./backups:/app/backups
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - C_FORCE_ROOT=1
    env_file:
      - .env.prod
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - logistics_network
    healthcheck:
      test: ["CMD", "celery", "-A", "config", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ============================================================================
  # CELERY BEAT - Task Scheduler
  # ============================================================================
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILD_ENV=production
    container_name: logistics_celery_beat
    command: |
      sh -c "
        echo 'Starting Celery beat scheduler...'
        celery -A config beat \
          --loglevel=info \
          --scheduler django_celery_beat.schedulers:DatabaseScheduler \
          --logfile=/app/logs/celery-beat.log
      "
    volumes:
      - ./logs:/app/logs
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    env_file:
      - .env.prod
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - logistics_network

  # ============================================================================
  # FLOWER - Celery Monitoring
  # ============================================================================
  flower:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILD_ENV=production
    container_name: logistics_flower
    command: |
      sh -c "
        echo 'Starting Flower monitoring...'
        celery -A config flower \
          --port=5555 \
          --basic_auth=admin:$$FLOWER_PASSWORD \
          --url_prefix=flower \
          --persistent=True \
          --db=/app/logs/flower.db \
          --logfile=/app/logs/flower.log
      "
    ports:
      - "5555:5555"
    volumes:
      - ./logs:/app/logs
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
      - PYTHONUNBUFFERED=1
    env_file:
      - .env.prod
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - logistics_network

  # ============================================================================
  # POSTGRESQL DATABASE - Main Database
  # ============================================================================
  db:
    image: postgres:15-alpine
    container_name: logistics_postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
      - ./backups/postgres:/backups
      - ./postgres/conf/postgresql.conf:/etc/postgresql/postgresql.conf
    environment:
      - POSTGRES_DB=${DB_NAME:-logistics_db}
      - POSTGRES_USER=${DB_USER:-logistics_user}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-secure_password}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=en_US.UTF-8 --lc-ctype=en_US.UTF-8
      - PGDATA=/var/lib/postgresql/data/pgdata
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - logistics_network
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "pg_isready -U ${DB_USER:-logistics_user} -d ${DB_NAME:-logistics_db}",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    command: |
      postgres -c 'max_connections=200'
               -c 'shared_buffers=256MB'
               -c 'effective_cache_size=1GB'
               -c 'work_mem=4MB'
               -c 'maintenance_work_mem=64MB'
               -c 'random_page_cost=1.1'
               -c 'temp_file_limit=2GB'
               -c 'log_min_duration_statement=1000'
               -c 'idle_in_transaction_session_timeout=10min'
               -c 'lock_timeout=1min'
               -c 'statement_timeout=10min'
               -c 'shared_preload_libraries=pg_stat_statements'

  # ============================================================================
  # REDIS - Cache & Message Broker
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: logistics_redis
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
    ports:
      - "6379:6379"
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: unless-stopped
    networks:
      - logistics_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    sysctls:
      - net.core.somaxconn=65535

  # ============================================================================
  # REDIS SENTINEL - High Availability (Optional)
  # ============================================================================
  redis-sentinel:
    image: redis:7-alpine
    container_name: logistics_redis_sentinel
    volumes:
      - ./redis/sentinel.conf:/usr/local/etc/redis/sentinel.conf
    ports:
      - "26379:26379"
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - logistics_network
    profiles:
      - ha

  # ============================================================================
  # BACKUP SERVICE - Database & Media Backups
  # ============================================================================
  backup:
    build:
      context: .
      dockerfile: Dockerfile.backup
    container_name: logistics_backup
    volumes:
      - postgres_data:/var/lib/postgresql/data:ro
      - media_volume:/app/media:ro
      - ./backups:/backups
    environment:
      - POSTGRES_DB=${DB_NAME:-logistics_db}
      - POSTGRES_USER=${DB_USER:-logistics_user}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-secure_password}
      - POSTGRES_HOST=db
      - BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-0 2 * * *}
      - BACKUP_RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-7}
    env_file:
      - .env.prod
    depends_on:
      db:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - logistics_network
    profiles:
      - backup

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/postgres
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/redis
  static_volume:
    driver: local
  media_volume:
    driver: local

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  logistics_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
